Resumo
Teoria: HPA, kube-controller e metrics APIs
Os comportamentos default do HPA s√£o controlados pelo kube-controller. Voc√™ pode filtrar por --horizontal-pod-autoscaler.

kube-controller-manager

per-pod resource metrics ‚Üí A forma default calculando a m√©dia do uso dos recursos de compute do pod como um todo (mesmo tendo mais de um container).

per-pod custom metrics ‚Üí Pode ser integrado com Prometheus para uso de outras m√©tricas.

Essas s√£o as APIs utilizadas pelo HPA:

metrics.k8s.io ‚Üí A principal, implementada pelo metrics-server.

custom.metrics.k8s.io

external.metrics.k8s.io

Ou seja, √© necess√°rio fazer o deploy do metrics-server:

GitHub - kubernetes-sigs/metrics-server: Scalable and efficient source of container resource metrics for Kubernetes built-in autoscaling pipelines.

HPA na pr√°tica
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-hpa
spec:
  metrics:
  - resource:
      name: cpu
      target:
        averageUtilization: 60
        type: Utilization
    type: Resource
  minReplicas: 1
	maxReplicas: 100
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx
  name: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx
        name: nginx
        resources:
          requests:
            cpu: 10m
          limits:
            cpu: 100m
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: nginx
  name: nginx
spec:
  ports:
  - port: 80
  selector:
    app: nginx
  type: ClusterIP
Simulando scaling e entendendo o algoritmo
Existe um algoritmo que √© usado para definir se ser√° escalado ou n√£o. Por exemplo:

desiredReplicas = ceil[currentReplicas * ( currentMetricValue / desiredMetricValue )]
Primeiro vamos entender a fun√ß√£o de ceil.

üí° The ceil() function computes the smallest integer that is greater than or equal to x.

Partindo do princ√≠pio que X seja 10.2, o valor ser√° 11.

currentReplicas √©, obviamente, o n√∫mero de replicas atual do deployment.

currentMetricValue √© a rela√ß√£o do uso atual em compara√ß√£o aos limites definidos no requests dentro do resources.

desiredMetricValue √© o ponto de escala que foi definido. No nosso exemplo foi 60%.

Vamos simular um cen√°rio‚Ä¶

Nosso requests √© de 10m (10 millicpus).

Rodando requisi√ß√µes em loop, o consumo do pod vai para 63m.

Com uma regra de 3‚Ä¶ (100*63)/10 = 630%.

A√≠ √© s√≥ aplicar a formula‚Ä¶ ceil[ 1 * ( 630 / 60 ) ] = 11

O deployment vai ser escalado para ~11 replicas.

Para simular‚Ä¶

$ kubectl run -it --image alpine demo sh

# apk add curl

# while true; do curl <svc>; done

Stabilization Window
Usado para evitar que os pods flutuem demais escalando e ‚Äúdeescalando‚Äù. Por default, depois que o pod escalar, vai demorar mais ou menos 5 minutos para estabilizar a janela, at√© que ent√£o parte deles possa ser deescalado.

√â poss√≠vel alterar a janela default no kube-controller ou sobrescrever o valor no deployment.

Horizontal Pod Autoscaling

Default values

Esses s√£o os valores para o HPA caso n√£o sejam modificados:

behavior:
  scaleDown:
    stabilizationWindowSeconds: 300
    policies:
    - type: Percent
      value: 100
      periodSeconds: 15
  scaleUp:
    stabilizationWindowSeconds: 0
    policies:
    - type: Percent
      value: 100
      periodSeconds: 15
    - type: Pods
      value: 4
      periodSeconds: 15
    selectPolicy: Max
Isso significa que‚Ä¶

Ap√≥s a stabilization window, o deployment pode ser deescalado em 100% ap√≥s o periodo de 15s.

N√£o h√° stabilization window para escalar.

Deve escalar de 4 em 4 pods a cada 15s. Ent√£o se no exemplo o n√∫mero de replicas era 11, ele faria 1 ‚Üí 5 ‚Üí 15s ‚Üí 9 ‚Üí 15s ‚Üí 11.

Ou, vai escalar 100% dos pods atuais em 15s (se tiver 1, escala somente 1).

Alterar comportamento do scale down

behavior:
  scaleDown:
    policies:
    - type: Percent
      value: 10
      periodSeconds: 60
Introdu√ß√£o e Deploy do VPA
autoscaler/vertical-pod-autoscaler at master ¬∑ kubernetes/autoscaler

O deploy do VPA √© bem simples:

$ git clone https://github.com/kubernetes/autoscaler
$ git checkout <tag>
$ cd autoscaler/vertical-pod-autoscaler
$ ./hack/vpa-up.sh
Voc√™ ver√° 3 componentes: admission, updater e recommender (mais sobre a arquitetura aqui).

admission ‚Üí Recebe os hooks quando um pod √© criado e faz o patch nos request/limits.

updater ‚Üí Atualiza os pods em tempo real fazendo o evicting.

recommender ‚Üí Verifica a utiliza√ß√£o de recursos e atualiza os VPAs.

Simulando scaling do VPA
Para entender bem, vamos aplicar o exemplo.

$ kubectl apply -f autoscaler/vertical-pod-autoscaler/examples/hamster.yaml
$ kubectl get po -w
$ kubectl get vpa -w
$ kubectl describe vpa <vpa>
O segredo por tr√°s do VPA √©: tanto o deployment quanto replicaset n√£o ser√£o editados.

No momento que um pod √© criado, h√° um mutatingwebhook que envia para o admission-controller, e ent√£o ocorre a mudan√ßa dos requests.

$ kubectl get mutatingwebhookconfigurations -A
$ kubectl get endpoints -n kube-system vpa-webhook
$ kubectl logs -n kube-system <admission pod> -f
E mesmo ap√≥s a cria√ß√£o do pod, ele ser√° atualizado por meio do updater.